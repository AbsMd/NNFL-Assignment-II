{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcetWGKN3oeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905bd705-6134-42a3-d843-ac196e8e4b9a"
      },
      "source": [
        "#deep layer stacked autoencoder based extreme learning machine\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "\n",
        "#Load data, shuffle and normalize\n",
        "mat_contents = loadmat('data5.mat')\n",
        "data = mat_contents['x']\n",
        "np.random.shuffle(data)\n",
        "\n",
        "\n",
        "def init_data():\n",
        "    X = np.array(data[ : , :-1], dtype = float)\n",
        "    y = np.array(data[ : , -1], dtype = int)\n",
        "    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n",
        "    return X, y\n",
        "\n",
        "#One-hot encoding\n",
        "X, y_ = init_data()\n",
        "y = np.zeros((len(y_), 2))\n",
        "for i in range(len(y_)):\n",
        "    if y_[i]==1:\n",
        "        y[i,1] = 1.0\n",
        "    elif y_[i]==0:\n",
        "        y[i,0] = 1.0\n",
        "\n",
        "#Hold out cross validn\n",
        "X_train, y_train = X[ :int(0.7 * len(X))], y[ :int(0.7 * len(X))]\n",
        "X_val, y_val = X[ int(0.7 * len(X)): ], y[ int(0.7 * len(X)): ]\n",
        "\n",
        "alpha = 0.5\n",
        "\n",
        "#Sigmoid activation function\n",
        "def sigmoid(x, derivative=False):\n",
        "        if (derivative == True):\n",
        "            return x * (1 - x)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#Tanh activation function\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, sizes):\n",
        "        \n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.W = {}\n",
        "        self.a = {}\n",
        "        self.b = {}\n",
        "        \n",
        "        #Initialize Weights\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.W[i] = np.random.randn(self.sizes[i-1], self.sizes[i])\n",
        "            \n",
        "        #Initialize biases\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.b[i] = np.random.randn(self.sizes[i], 1)\n",
        "        \n",
        "        #Initialize activations\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.a[i] = np.zeros([self.sizes[i], 1])\n",
        "        \n",
        "    #Forward pass to compute scores\n",
        "    def forward_pass(self, X):\n",
        "        \n",
        "        self.a[0] = X\n",
        "        \n",
        "        for i in range(1, self.num_layers):\n",
        "            self.a[i] = sigmoid(np.dot(self.W[i].T, self.a[i-1]) + self.b[i])\n",
        "\n",
        "        return self.a[self.num_layers-1] \n",
        "    \n",
        "    #Backward pass to update weights\n",
        "    def backward_pass(self, X, Y, output):\n",
        "        \n",
        "        self.d = {}\n",
        "        self.d_output = (Y - output) * sigmoid(output, derivative=True)\n",
        "        self.d[self.num_layers-1] = self.d_output\n",
        "        \n",
        "        #Derivatives of the layers wrt loss\n",
        "        for i in range(self.num_layers-1, 1, -1):\n",
        "            self.d[i-1] = np.dot(self.W[i], self.d[i]) * sigmoid(self.a[i-1], derivative=True)\n",
        "        \n",
        "        #Updating weights\n",
        "        for i in range(1, self.num_layers-1):\n",
        "            self.W[i] += alpha * np.dot(self.a[i-1], self.d[i].T)\n",
        "            \n",
        "        #Updating biases\n",
        "        for i in range(1, self.num_layers-1):\n",
        "            self.b[i] += alpha * self.d[i]\n",
        "\n",
        "    #Training helper function   \n",
        "    def train(self, X, Y):\n",
        "        X = np.reshape(X, (len(X), 1))\n",
        "        output = self.forward_pass(X)\n",
        "        self.backward_pass(X, Y, output)\n",
        "\n",
        "    #Get weights    \n",
        "    def get_W(self):\n",
        "        return self.W\n",
        "    \n",
        "    #Load specified weights\n",
        "    def load_W(self, W):\n",
        "        self.W = W\n",
        "\n",
        "    #Scores computation for given input    \n",
        "    def get_a(self, x):\n",
        "        x = np.reshape(x, (len(x), 1))\n",
        "        self.forward_pass(x)\n",
        "        return self.a\n",
        "    \n",
        "    #Helper function for autoencoder chaining\n",
        "    def load_a(self, a):\n",
        "        self.a = a\n",
        "\n",
        "    \n",
        "\n",
        "#cost function\n",
        "def calc_cost(NN,x ,y):\n",
        "    \n",
        "    cost = 0\n",
        "    for i in range(len(x)):\n",
        "        x_ = np.reshape(x[i], (len(x[i]), 1))\n",
        "        cost += 0.5 / len(x) * np.sum((y[i] - NN.forward_pass(x_)) ** 2)\n",
        "    \n",
        "    return cost\n",
        "\n",
        "#Network initialization\n",
        "autoencoder1 = NeuralNetwork([72, 60, 72])\n",
        "autoencoder2 = NeuralNetwork([60,40,60])\n",
        "\n",
        "#AE 1 pretraining\n",
        "for i in range(500):\n",
        "    for j, row in enumerate(X_train):\n",
        "        row = np.reshape(row, (72,1))\n",
        "        autoencoder1.train(row, row)\n",
        "        \n",
        "    cost = calc_cost(autoencoder1, X_train, X_train)\n",
        "    if ((i%100)-1 == 0):\n",
        "      print(\"Epoch {}, cost {}\".format(i, cost))\n",
        "    \n",
        "#Scores for AE 1\n",
        "autoencoder2_input = []\n",
        "\n",
        "for row in X_train:\n",
        "    autoencoder2_input.append(autoencoder1.get_a(row)[1])\n",
        "\n",
        "autoencoder2_input = np.array(autoencoder2_input)\n",
        "\n",
        "\n",
        "#AE 2 pretraining\n",
        "for i in range(500):\n",
        "    for j, row in enumerate(autoencoder2_input):\n",
        "        row = np.reshape(row, (60,1))\n",
        "        autoencoder2.train(row, row)\n",
        "        \n",
        "    cost = calc_cost(autoencoder2, autoencoder2_input, autoencoder2_input)\n",
        "    if ((i%100)-1 == 0):\n",
        "      print(\"Epoch {}, cost {}\".format(i, cost))\n",
        "\n",
        "#Inputs to ELM\n",
        "\n",
        "elm_input = []\n",
        "for row in autoencoder2_input:\n",
        "    elm_input.append(autoencoder2.get_a(row)[1])   \n",
        "elm_input = np.array(elm_input)\n",
        "\n",
        "#parameters for ELM\n",
        "elm_neurons = 300\n",
        "output_neurons = 2\n",
        "W_elm = np.random.randn(elm_input.shape[1], elm_neurons)\n",
        "\n",
        "#ELM Training\n",
        "np.random.seed(1)\n",
        "elm_input = np.reshape(elm_input, (1503, 40))\n",
        "H = np.matmul(elm_input, W_elm)\n",
        "H = tanh(H)\n",
        "H_inv = np.linalg.pinv(H)\n",
        "W_final = np.matmul(H_inv, y_train) \n",
        "\n",
        "#Testing on validation dataset\n",
        "#AE 1 forward pass\n",
        "layer1_out = []\n",
        "\n",
        "for i, row in enumerate(X_val):\n",
        "    act = autoencoder1.get_a(row)[1]\n",
        "    layer1_out.append(act)\n",
        "    \n",
        "layer1_out = np.array(layer1_out)\n",
        "layer1_out = np.reshape(layer1_out, (645, 60))\n",
        "\n",
        "#AE 2 forward pass\n",
        "layer2_out = []\n",
        "\n",
        "for i, row in enumerate(layer1_out):\n",
        "    act = autoencoder2.get_a(row)[1]\n",
        "    layer2_out.append(act)\n",
        "    \n",
        "layer2_out = np.array(layer2_out)\n",
        "layer2_out = np.reshape(layer2_out, (645, 40))\n",
        "\n",
        "#ELM forward pass\n",
        "H_T = np.matmul(layer2_out, W_elm)\n",
        "H_T = tanh(H_T)\n",
        "y_pred = np.matmul(H_T, W_final)\n",
        "\n",
        "tp, tn, fp, fn = 0,0,0,0\n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    \n",
        "    if np.argmax(y_pred[i]) == 1 and np.argmax(y_val[i]) == 1:\n",
        "        tp += 1\n",
        "    elif np.argmax(y_pred[i]) == 0 and np.argmax(y_val[i]) == 0:\n",
        "        tn += 1\n",
        "    elif np.argmax(y_pred[i]) == 1 and np.argmax(y_val[i]) == 0:\n",
        "        fp += 1\n",
        "    elif np.argmax(y_pred[i]) == 0 and np.argmax(y_val[i]) == 1:\n",
        "        fn += 1\n",
        "\n",
        "print(tp, fp)\n",
        "print(fn, tn)\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)\n",
        "#Best accuracy recorded=8.24096.."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, cost 3015.961558532477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 101, cost 2960.3172801970854\n",
            "Epoch 201, cost 2959.7994252450617\n",
            "Epoch 301, cost 2960.6133305654485\n",
            "Epoch 401, cost 2960.6720379608028\n",
            "Epoch 1, cost 5.210162810659542\n",
            "Epoch 101, cost 4.571406517090676\n",
            "Epoch 201, cost 4.496644984556381\n",
            "Epoch 301, cost 4.481221904486046\n",
            "Epoch 401, cost 4.465124914598747\n",
            "249 62\n",
            "78 256\n",
            "accuracy =  0.7829457364341085 sensitivity =  0.7614678899082569 specificity =  0.8050314465408805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}