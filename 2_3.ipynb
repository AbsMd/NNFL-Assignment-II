{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "UwaB7ELJaIO2",
        "outputId": "24da3a8a-7a77-4a01-b8f2-7f9659bd75c7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "\n",
        "mat_contents = loadmat('data5.mat')\n",
        "data = mat_contents['x']\n",
        "np.random.shuffle(data)\n",
        "\n",
        "def init_data(): \n",
        "    X = np.array(data[:2148, :-1], dtype = float)\n",
        "    y = np.array(data[:2148, -1], dtype = int)\n",
        "    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n",
        "    return X, y\n",
        "def relu_forward(x):\n",
        "    a = x\n",
        "    a[a<=0] = 0\n",
        "    theta = x\n",
        "    return a, theta\n",
        "def relu_backward(dout, theta):\n",
        "    x = theta\n",
        "    dx = None\n",
        "    dx = np.ones(x.shape)\n",
        "    dx[x<=0] = 0\n",
        "    dx = dx * dout\n",
        "    return dx\n",
        "def collinear_forward(x, w, b):\n",
        "    z = x.dot(w) + b\n",
        "    theta = (x, w, b)\n",
        "    return z, theta\n",
        "def collinear_backward(dout, theta):\n",
        "    x, w, b = theta\n",
        "    db = np.sum(dout, axis = 0)\n",
        "    dw = x.T.dot(dout)\n",
        "    dx = dout.dot(w.T)\n",
        "    return dx, dw, db\n",
        "\n",
        "print('iterations per fold: 4000')\n",
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, input_size, h1, h2, num_classes, std=1e-4):\n",
        "        self.W1 = std * np.random.randn(input_size, h1)\n",
        "        self.b1 = np.zeros(h1)\n",
        "        self.W2 = std * np.random.randn(h1, h2)\n",
        "        self.b2 = np.zeros(h2)\n",
        "        self.W3 = std * np.random.randn(h2, num_classes)\n",
        "        self.b3 = np.zeros(num_classes)\n",
        "\n",
        "    def cost(self, X, y = None, lambda1 = 0.0): \n",
        "        N, D = X.shape\n",
        "        scores = None\n",
        "        z1, af_theta1 = collinear_forward(X, self.W1, self.b1)\n",
        "        h1, relu_theta1 = relu_forward(z1)\n",
        "        z2, af_theta2 = collinear_forward(h1, self.W2, self.b2)\n",
        "        h2, relu_theta2 = relu_forward(z2)\n",
        "        z3, af_theta3 = collinear_forward(h2, self.W3, self.b3)\n",
        "        scores = z3\n",
        "\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        cost = None\n",
        "        scores -= scores.max()\n",
        "        scores_exp = np.exp(scores)\n",
        "        correct_scores = scores[range(N), y]\n",
        "        correct_scores_exp = np.exp(correct_scores)\n",
        "        cost = np.sum(-np.log(correct_scores_exp / np.sum(scores_exp, axis = 1))) / N\n",
        "        cost += 0.5 * lambda1 * (np.sum(self.W1 * self.W1) + np.sum(self.W2 * self.W2) + np.sum(self.W3 * self.W3))\n",
        "\n",
        "        num = correct_scores_exp\n",
        "        denom = np.sum(scores_exp, axis = 1)\n",
        "        mask = (np.exp(z3)/denom.reshape(scores.shape[0],1))\n",
        "        mask[range(N),y] = -(denom - num)/denom\n",
        "        mask /= N\n",
        "        dz3 = mask\n",
        "\n",
        "        dh2, dw3, db3 = collinear_backward(dz3, af_theta3)\n",
        "        dz2 = relu_backward(dh2, relu_theta2)\n",
        "        dh1, dw2, db2 = collinear_backward(dz2, af_theta2)\n",
        "        dz1 = relu_backward(dh1, relu_theta1)\n",
        "        dx, dw1, db1 = collinear_backward(dz1, af_theta1)\n",
        "        \n",
        "        dw3 = dw3 + lambda1 * self.W3\n",
        "        dw2 = dw2 + lambda1 * self.W2\n",
        "        dw1 = dw1 + lambda1 * self.W1\n",
        "\n",
        "        wgrad = (dw1, dw2, dw3)\n",
        "        bgrad = (db1, db2, db3)\n",
        "\n",
        "        return cost, wgrad, bgrad\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, alpha=0.001, alpha_decay=0.95, lambda1=5e-6, num_iters=100, batch_size=200):\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "        cost_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for iter in range(num_iters):\n",
        "\n",
        "            ind = np.random.choice(num_train, batch_size)\n",
        "            X_batch = X[ind,:]\n",
        "            y_batch = y[ind]\n",
        "            \n",
        "            cost, wgrad, bgrad = self.cost(X_batch, y = y_batch, lambda1 = lambda1)\n",
        "            cost_history.append(cost)\n",
        "\n",
        "            dw1, dw2, dw3 = wgrad\n",
        "            db1, db2, db3 = bgrad\n",
        "\n",
        "            self.W1 -= alpha * dw1\n",
        "            self.W2 -= alpha * dw2\n",
        "            self.W3 -= alpha * dw3\n",
        "            self.b1 -= alpha * db1\n",
        "            self.b2 -= alpha * db2\n",
        "            self.b3 -= alpha * db3\n",
        "\n",
        "\n",
        "            if iter % 1000 == 0:\n",
        "                print('iteration %d : cost %f' % (iter, cost))\n",
        "\n",
        "\n",
        "            if iter % iterations_per_epoch == 0:\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "                alpha *= alpha_decay\n",
        "\n",
        "\n",
        "        return {'cost_history' : cost_history, 'train_acc_history' : train_acc_history, 'val_acc_history' : val_acc_history}\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.argmax(self.cost(X), axis = 1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "input_size = 72\n",
        "hidden_size1 = 30\n",
        "hidden_size2 = 30\n",
        "num_classes = 2\n",
        "num_inputs = 1790\n",
        "std = 0.1\n",
        "alpha = 0.3\n",
        "batch_size = 1024\n",
        "lambda1 = 0.01\n",
        "num_iters = 4000\n",
        "\n",
        "X_tot, y_tot = init_data()\n",
        "\n",
        "train_acc , val_acc = 0, 0\n",
        "costs = np.empty((6, num_iters))\n",
        "val_accs = []\n",
        "train_accs = []\n",
        "\n",
        "for k in range(6):\n",
        "    \n",
        "    X = X_tot[0 : 1790]\n",
        "    y = y_tot[0 : 1790]\n",
        "    X_val = X_tot[1790 :]\n",
        "    y_val = y_tot[1790 :]\n",
        "    \n",
        "    Net = MLP(input_size, hidden_size1, hidden_size2, num_classes, std)\n",
        "    print(\"Validation fold : \" , k + 1)\n",
        "    stats = Net.train(X, y, X_val, y_val, num_iters=num_iters, alpha=alpha, batch_size=batch_size, lambda1=0.0)\n",
        "    costs[k] = np.asarray(stats['cost_history'])\n",
        "    val_accs = np.asarray(stats['val_acc_history'])\n",
        "    train_accs = np.asarray(stats['train_acc_history'])\n",
        "    train_acc += train_accs\n",
        "    val_acc += val_accs\n",
        "\n",
        "    X_tot[0 : 358] = X_val\n",
        "    X_tot[358 : ] = X\n",
        "    y_tot[0 : 358] = y_val\n",
        "    y_tot[358 : ] = y\n",
        "\n",
        "train_acc = train_acc / 6\n",
        "val_acc = val_acc / 6\n",
        "\n",
        "print('\\nTraining accuracy: ', train_acc[-1], 'Validn Accuracy: ',val_acc[-1])\n",
        "cost_hist = np.mean(costs, axis = 0)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(cost_hist, 'r-')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('cost')\n",
        "\n",
        "y_pred = Net.predict(X_val)\n",
        "tp, tn, fp, fn = 0, 0, 0, 0\n",
        "for i in range(len(y_val)):\n",
        "    if y_pred[i] == 0 and  y_val[i] == 0:\n",
        "        tn += 1\n",
        "    elif y_pred[i] == 1 and  y_val[i] == 0:\n",
        "        fp += 1\n",
        "    elif y_pred[i] == 0 and  y_val[i] == 1:\n",
        "        fn += 1\n",
        "    elif y_pred[i] == 1 and  y_val[i] == 1:\n",
        "        tp += 1\n",
        "print('\\nPred   1  0')\n",
        "print('Ac 1  ',tp, fp)\n",
        "print('   0  ',fn, tn)\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(\"\\naccuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterations per fold: 4000\n",
            "Validation fold :  1\n",
            "iteration 0 : cost 0.695283\n",
            "iteration 1000 : cost 0.001124\n",
            "iteration 2000 : cost 0.000353\n",
            "iteration 3000 : cost 0.000177\n",
            "Validation fold :  2\n",
            "iteration 0 : cost 0.706589\n",
            "iteration 1000 : cost 0.000891\n",
            "iteration 2000 : cost 0.000326\n",
            "iteration 3000 : cost 0.000194\n",
            "Validation fold :  3\n",
            "iteration 0 : cost 0.695811\n",
            "iteration 1000 : cost 0.000690\n",
            "iteration 2000 : cost 0.000343\n",
            "iteration 3000 : cost 0.000220\n",
            "Validation fold :  4\n",
            "iteration 0 : cost 0.674739\n",
            "iteration 1000 : cost 0.000993\n",
            "iteration 2000 : cost 0.000364\n",
            "iteration 3000 : cost 0.000184\n",
            "Validation fold :  5\n",
            "iteration 0 : cost 0.681118\n",
            "iteration 1000 : cost 0.000791\n",
            "iteration 2000 : cost 0.000320\n",
            "iteration 3000 : cost 0.000198\n",
            "Validation fold :  6\n",
            "iteration 0 : cost 0.701590\n",
            "iteration 1000 : cost 0.001128\n",
            "iteration 2000 : cost 0.000374\n",
            "iteration 3000 : cost 0.000194\n",
            "\n",
            "Training accuracy:  1.0 Validn Accuracy:  0.9585661080074487\n",
            "\n",
            "Pred   1  0\n",
            "Ac 1   180 0\n",
            "   0   0 178\n",
            "\n",
            "accuracy =  1.0 sensitivity =  1.0 specificity =  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}