{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "UwaB7ELJaIO2",
        "outputId": "762c589c-9838-4858-a467-1b0a989bd672"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "\n",
        "mat_contents = loadmat('data5.mat')\n",
        "data = mat_contents['x']\n",
        "np.random.shuffle(data)\n",
        "\n",
        "def init_data(): \n",
        "    X = np.array(data[:2148, :-1], dtype = float)\n",
        "    y = np.array(data[:2148, -1], dtype = int)\n",
        "    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n",
        "    return X, y\n",
        "def relu_forward(x):\n",
        "    a = x\n",
        "    a[a<=0] = 0\n",
        "    theta = x\n",
        "    return a, theta\n",
        "def relu_backward(dout, theta):\n",
        "    x = theta\n",
        "    dx = None\n",
        "    dx = np.ones(x.shape)\n",
        "    dx[x<=0] = 0\n",
        "    dx = dx * dout\n",
        "    return dx\n",
        "def collinear_forward(x, w, b):\n",
        "    z = x.dot(w) + b\n",
        "    theta = (x, w, b)\n",
        "    return z, theta\n",
        "def collinear_backward(dout, theta):\n",
        "    x, w, b = theta\n",
        "    db = np.sum(dout, axis = 0)\n",
        "    dw = x.T.dot(dout)\n",
        "    dx = dout.dot(w.T)\n",
        "    return dx, dw, db\n",
        "\n",
        "print('iterations per fold: 5000')\n",
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, input_size, h1, h2, num_classes, std=1e-4):\n",
        "        self.W1 = std * np.random.randn(input_size, h1)\n",
        "        self.b1 = np.zeros(h1)\n",
        "        self.W2 = std * np.random.randn(h1, h2)\n",
        "        self.b2 = np.zeros(h2)\n",
        "        self.W3 = std * np.random.randn(h2, num_classes)\n",
        "        self.b3 = np.zeros(num_classes)\n",
        "\n",
        "    def cost(self, X, y = None, lambda1 = 0.0): \n",
        "        N, D = X.shape\n",
        "        scores = None\n",
        "        z1, af_theta1 = collinear_forward(X, self.W1, self.b1)\n",
        "        h1, relu_theta1 = relu_forward(z1)\n",
        "        z2, af_theta2 = collinear_forward(h1, self.W2, self.b2)\n",
        "        h2, relu_theta2 = relu_forward(z2)\n",
        "        z3, af_theta3 = collinear_forward(h2, self.W3, self.b3)\n",
        "        scores = z3\n",
        "\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        cost = None\n",
        "        scores -= scores.max()\n",
        "        scores_exp = np.exp(scores)\n",
        "        correct_scores = scores[range(N), y]\n",
        "        correct_scores_exp = np.exp(correct_scores)\n",
        "        cost = np.sum(-np.log(correct_scores_exp / np.sum(scores_exp, axis = 1))) / N\n",
        "        cost += 0.5 * lambda1 * (np.sum(self.W1 * self.W1) + np.sum(self.W2 * self.W2) + np.sum(self.W3 * self.W3))\n",
        "\n",
        "        num = correct_scores_exp\n",
        "        denom = np.sum(scores_exp, axis = 1)\n",
        "        mask = (np.exp(z3)/denom.reshape(scores.shape[0],1))\n",
        "        mask[range(N),y] = -(denom - num)/denom\n",
        "        mask /= N\n",
        "        dz3 = mask\n",
        "\n",
        "        dh2, dw3, db3 = collinear_backward(dz3, af_theta3)\n",
        "        dz2 = relu_backward(dh2, relu_theta2)\n",
        "        dh1, dw2, db2 = collinear_backward(dz2, af_theta2)\n",
        "        dz1 = relu_backward(dh1, relu_theta1)\n",
        "        dx, dw1, db1 = collinear_backward(dz1, af_theta1)\n",
        "        \n",
        "        dw3 = dw3 + lambda1 * self.W3\n",
        "        dw2 = dw2 + lambda1 * self.W2\n",
        "        dw1 = dw1 + lambda1 * self.W1\n",
        "\n",
        "        wgrad = (dw1, dw2, dw3)\n",
        "        bgrad = (db1, db2, db3)\n",
        "\n",
        "        return cost, wgrad, bgrad\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, alpha=0.001, alpha_decay=0.95, lambda1=5e-6, num_iters=100, batch_size=200):\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "        cost_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for iter in range(num_iters):\n",
        "\n",
        "            ind = np.random.choice(num_train, batch_size)\n",
        "            X_batch = X[ind,:]\n",
        "            y_batch = y[ind]\n",
        "            \n",
        "            cost, wgrad, bgrad = self.cost(X_batch, y = y_batch, lambda1 = lambda1)\n",
        "            cost_history.append(cost)\n",
        "\n",
        "            dw1, dw2, dw3 = wgrad\n",
        "            db1, db2, db3 = bgrad\n",
        "\n",
        "            self.W1 -= alpha * dw1\n",
        "            self.W2 -= alpha * dw2\n",
        "            self.W3 -= alpha * dw3\n",
        "            self.b1 -= alpha * db1\n",
        "            self.b2 -= alpha * db2\n",
        "            self.b3 -= alpha * db3\n",
        "\n",
        "\n",
        "            if iter % 1000 == 0:\n",
        "                print('iteration %d : cost %f' % (iter, cost))\n",
        "\n",
        "\n",
        "            if iter % iterations_per_epoch == 0:\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "                alpha *= alpha_decay\n",
        "\n",
        "\n",
        "        return {'cost_history' : cost_history, 'train_acc_history' : train_acc_history, 'val_acc_history' : val_acc_history}\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.argmax(self.cost(X), axis = 1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "input_size = 72\n",
        "hidden_size1 = 30\n",
        "hidden_size2 = 30\n",
        "num_classes = 2\n",
        "num_inputs = 1790\n",
        "std = 0.1\n",
        "alpha = 0.3\n",
        "batch_size = 1024\n",
        "lambda1 = 0.01\n",
        "num_iters = 5000\n",
        "\n",
        "X_tot, y_tot = init_data()\n",
        "\n",
        "train_acc , val_acc = 0, 0\n",
        "costs = np.empty((6, num_iters))\n",
        "val_accs = []\n",
        "train_accs = []\n",
        "\n",
        "for k in range(6):\n",
        "    \n",
        "    X = X_tot[0 : 1790]\n",
        "    y = y_tot[0 : 1790]\n",
        "    X_val = X_tot[1790 :]\n",
        "    y_val = y_tot[1790 :]\n",
        "    \n",
        "    Net = MLP(input_size, hidden_size1, hidden_size2, num_classes, std)\n",
        "    print(\"Validation fold : \" , k + 1)\n",
        "    stats = Net.train(X, y, X_val, y_val, num_iters=num_iters, alpha=alpha, batch_size=batch_size, lambda1=0.0)\n",
        "    costs[k] = np.asarray(stats['cost_history'])\n",
        "    val_accs = np.asarray(stats['val_acc_history'])\n",
        "    train_accs = np.asarray(stats['train_acc_history'])\n",
        "    train_acc += train_accs\n",
        "    val_acc += val_accs\n",
        "\n",
        "    X_tot[0 : 358] = X_val\n",
        "    X_tot[358 : ] = X\n",
        "    y_tot[0 : 358] = y_val\n",
        "    y_tot[358 : ] = y\n",
        "\n",
        "train_acc = train_acc / 6\n",
        "val_acc = val_acc / 6\n",
        "\n",
        "print('Training accuracy: ', train_acc[-1], 'Validn Accuracy: ',val_acc[-1])\n",
        "cost_hist = np.mean(costs, axis = 0)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(cost_hist)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('cost')\n",
        "\n",
        "y_pred = Net.predict(X_val)\n",
        "tp, tn, fp, fn = 0, 0, 0, 0\n",
        "for i in range(len(y_val)):\n",
        "    if y_pred[i] == 0 and  y_val[i] == 0:\n",
        "        tn += 1\n",
        "    elif y_pred[i] == 1 and  y_val[i] == 0:\n",
        "        fp += 1\n",
        "    elif y_pred[i] == 0 and  y_val[i] == 1:\n",
        "        fn += 1\n",
        "    elif y_pred[i] == 1 and  y_val[i] == 1:\n",
        "        tp += 1\n",
        "print('Pred   1  0')\n",
        "print('Ac 1  ',tp, fp)\n",
        "print('   0  ',fn, tn)\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterations per fold: 5000\n",
            "Validation fold :  1\n",
            "iteration 0 : cost 0.687705\n",
            "iteration 1000 : cost 0.000886\n",
            "iteration 2000 : cost 0.000476\n",
            "iteration 3000 : cost 0.000228\n",
            "iteration 4000 : cost 0.000139\n",
            "Validation fold :  2\n",
            "iteration 0 : cost 0.695383\n",
            "iteration 1000 : cost 0.001091\n",
            "iteration 2000 : cost 0.000381\n",
            "iteration 3000 : cost 0.000236\n",
            "iteration 4000 : cost 0.000105\n",
            "Validation fold :  3\n",
            "iteration 0 : cost 0.709530\n",
            "iteration 1000 : cost 0.001150\n",
            "iteration 2000 : cost 0.000342\n",
            "iteration 3000 : cost 0.000191\n",
            "iteration 4000 : cost 0.000210\n",
            "Validation fold :  4\n",
            "iteration 0 : cost 0.703115\n",
            "iteration 1000 : cost 0.000979\n",
            "iteration 2000 : cost 0.000320\n",
            "iteration 3000 : cost 0.000193\n",
            "iteration 4000 : cost 0.000109\n",
            "Validation fold :  5\n",
            "iteration 0 : cost 0.705510\n",
            "iteration 1000 : cost 0.000821\n",
            "iteration 2000 : cost 0.000295\n",
            "iteration 3000 : cost 0.000225\n",
            "iteration 4000 : cost 0.000130\n",
            "Validation fold :  6\n",
            "iteration 0 : cost 0.701218\n",
            "iteration 1000 : cost 0.000945\n",
            "iteration 2000 : cost 0.000308\n",
            "iteration 3000 : cost 0.000264\n",
            "iteration 4000 : cost 0.000141\n",
            "Training accuracy:  1.0 Validn Accuracy:  0.9562383612662942\n",
            "Pred   1  0\n",
            "Ac 1   168 0\n",
            "   0   0 190\n",
            "accuracy =  1.0 sensitivity =  1.0 specificity =  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}