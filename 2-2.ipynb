{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNlCp3joLXI2uwkjBt/MQY+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mW21ekwClQLi","executionInfo":{"status":"ok","timestamp":1606531363401,"user_tz":-330,"elapsed":1650,"user":{"displayName":"M ABHINAVA","photoUrl":"","userId":"12313623155509412289"}},"outputId":"9c0f6574-0098-4406-e6d8-5d855ead68bf"},"source":["import math \n","import numpy as np\n","import pandas as pd\n","\n","def get_column(data,c):\n","    column = []\n","    for i in range(len(data)):\n","        column.append(data[i][c])\n","    return column\n","\n","def set(a):\n","    for i in range(len(a)):\n","        if a[i] >= 0.5:\n","          a[i] = 1;\n","        else:\n","          a[i] = 0;\n","    return a\n","\n","def sigmoid(x):\n","    return 1 / (1 + math.exp(-x))\n","\n","def poly_kernel(X1, X2):\n","    k = np.matmul(X1 , np.transpose(X2))\n","    k = k + 1\n","    k = np.power(k,3)\n","    return k\n","\n","def kernel_perceptron_pred(w, y, K, i):\n","    hyp = np.multiply(np.multiply(w, y), K[i,:])\n","    hyp = sum(hyp)\n","    if (hyp > 0):\n","          hyp = 1\n","    else:\n","          hyp = -1\n","    return hyp\n","\n","def kernel_perceptron(x_tr, y_tr, iterations):\n","    m = x_tr.shape[0]\n","    n = x_tr.shape[1]\n","    alpha = np.zeros(m)\n","    w = np.zeros(n)\n","    K = poly_kernel(x_tr,x_tr)\n","    hyp = np.zeros(m)\n","    for iter in range(iterations):\n","      for i in range(m):\n","        hyp[i] = kernel_perceptron_pred(alpha, y_tr, K, i)\n","        if (hyp[i]!=y_tr[i]):\n","          alpha[i] = alpha[i] + 1\n","    print('alpha: ' + str(alpha) + '\\n')\n","    for i in range(m):\n","      w += (alpha[i] * y_tr[i]) * x[i,:]\n","    print('weights: ' +str(w))\n","    return w\n","\n","#Normalization\n","def normalize(data):\n","    normalized_data = data\n","    for i in range(5):\n","        sd = np.std(get_column(data,i))\n","        mean = np.mean(get_column(data,i))\n","        for j in range(len(data)):\n","            normalized_data[j][i] = (data[j][i]-mean)/sd\n","    return normalized_data\n","\n","#Data\n","data = pd.read_excel('data55.xlsx',header=None)\n","data = data.sample(frac=1).reset_index(drop=True)\n","#print(data)\n","#print(type(data))\n","data = data.to_numpy()\n","y = data[:,-1]\n","data = normalize(data)\n","x = data[:,:-1]\n","x1 = np.ones(len(x))\n","x = np.column_stack((x1, x))\n","\n","iterations = 100\n","alpha = 0.01\n","\n","#Split into testing and training sets\n","train_size = int(0.7 * len(x))\n","val_limit = int(0.8*len(x))\n","x_tr = x[:train_size]\n","x_val = x[train_size:val_limit]\n","x_ts = x[val_limit:]\n","y_tr = y[:train_size]\n","y_val = y[train_size:val_limit]\n","y_ts = y[val_limit:]\n","\n","yp = np.zeros(len(x_tr))\n","m = x_tr.shape[0]\n","\n","w = kernel_perceptron(x_tr,y_tr, iterations)\n","K = poly_kernel(x_tr, x_tr)\n","\n","x = x_val\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(x[i], np.transpose(w))\n","    yp[i] = sigmoid(yp[i])\n","y_val = set(y_val)\n","yp = set(yp)\n","print('predicted: ' +str(yp) + '\\n')\n","y_actual = pd.Series(y_val, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp1 = confmat[1][1]\n","tn1 = confmat[0][0]\n","fp1 = confmat[0][1]\n","fn1 = confmat[1][0]\n","\n","validation_accuracy = (tp1+tn1)/(tp1+tn1+fp1+fn1)\n","print('Validation Accuracy : ' + str(validation_accuracy) + '\\n')\n","\n","x = x_ts\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(x[i], np.transpose(w))\n","    yp[i] = sigmoid(yp[i])\n","y_ts = set(y_ts)\n","yp = set(yp)\n","print('predicted: ' +str(yp) + '\\n')\n","y_actual = pd.Series(y_ts, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp = confmat[1][1]\n","tn = confmat[0][0]\n","fp = confmat[0][1]\n","fn = confmat[1][0]\n","\n","test_accuracy = (tp+tn)/(tp+tn+fp+fn)\n","sensitivity = tp/(tp+fn)\n","specificity = tn/(tn+fp)\n","\n","print('\\nTest Accuracy : ' + str(test_accuracy))\n","print('sensitivity : ' + str(sensitivity))\n","print('specificity : ' + str(specificity))\n","\n","\n","#Since there are only 100 instances(10 instances for val and 20 for test) \n","#and data.sample sometimes doesn't shuffle the data properly,\n","#it is possible to get an error in confmat index, please run till the shuffling is acceptable"],"execution_count":null,"outputs":[{"output_type":"stream","text":["alpha: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","weights: [ 0.          0.78310769 -1.68893681  1.52639599  1.5973046 ]\n","predicted: [0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n","\n","Predicted  0  1\n","Actual         \n","0.0        6  0\n","1.0        0  4\n","Validation Accuracy : 1.0\n","\n","predicted: [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n","\n","Predicted   0   1\n","Actual           \n","0.0        10   0\n","1.0         0  10\n","\n","Test Accuracy : 1.0\n","sensitivity : 1.0\n","specificity : 1.0\n"],"name":"stdout"}]}]}