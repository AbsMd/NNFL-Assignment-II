{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqf4mv9xdw8GLpOxU7XyS0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"id":"FisdEQPlB61K","executionInfo":{"status":"error","timestamp":1606528147467,"user_tz":-330,"elapsed":2806,"user":{"displayName":"M ABHINAVA","photoUrl":"","userId":"12313623155509412289"}},"outputId":"85c8efd4-a7af-465f-ef9a-0575bb83361a"},"source":["import math \n","import numpy as np\n","import pandas as pd\n","\n","def poly_kernel(X1, X2):\n","    k = np.matmul(X1 , np.transpose(X2))\n","    k = k + 1\n","    k = np.power(k,3)\n","    return k\n","\n","def kernel_perceptron_pred(w, y, K, i):\n","        hyp = np.multiply(np.multiply(alpha , y) , K[i,:])\n","        hyp = sum(hyp)\n","        if (hyp[i] >= 0.5):\n","          hyp[i] = 1\n","        else:\n","          hyp[i] = 0\n","    return hyp\n","\n","def kernel_perceptron(x_tr, y_tr, iterations):\n","    m = x_tr.shape[0]\n","    n = x_tr.shape[1]\n","    alpha = np.zeros(m)\n","    w = np.zeros(n)\n","    K = poly_kernel(x_tr,x_tr)\n","    print(k.shape)\n","    print(type(k))\n","    pred = np.zeros(m)\n","    hyp = 0\n","    for iter in range(iterations):\n","      hyp = kernel_perceptron_pred(alpha, y_tr, K, i)\n","      for i in range(m):\n","        if (hyp[i]!=y_tr[i]):\n","          alpha[i] = alpha[i] + 1\n","    print(alpha)\n","    for i in range(m):\n","      w += np.matmul(np.matmul(alpha),x[i:])\n","    return alpha\n","\n","#Normalization\n","def normalize(data):\n","    normalized_data = data\n","    for i in range(5):\n","        maxval = max(get_column(data,i))\n","        minval = min(get_column(data,i))\n","        for j in range(len(data)):\n","            normalized_data[j][i] = (data[j][i]-minval)/(maxval-minval)\n","    return normalized_data\n","\n","#Data\n","data = pd.read_excel('data55.xlsx',header=None)\n","data = data.sample(frac=1).reset_index(drop=True)\n","#print(data)\n","#print(type(data))\n","data = data.to_numpy()\n","y = data[:,-1]\n","#data = normalize(data)\n","x = data[:,:-1]\n","print(data.shape)\n","x1 = np.ones(len(x))\n","x = np.column_stack((x1, x))\n","\n","iterations = 1000\n","alpha = 0.01\n","\n","#Split into testing and training sets\n","train_size = int(0.7 * len(x))\n","val_limit = int(0.8*len(x))\n","x_tr = x[:train_size]\n","x_val = x[train_size:val_limit]\n","x_ts = x[val_limit:]\n","y_tr = y[:train_size]\n","y_val = y[train_size:val_limit]\n","y_ts = y[val_limit:]\n","\n","alpha = kernel_perceptron(x_tr,y_tr, iterations)\n","K = poly_kernel(x_tr, x_tr)\n","for i in range(m):\n","    train_pred[i] = kernel_perceptron_pred(w, y_tr, K, i)\n","print(w)\n","\n","train_accuracy = train_pred - y_tr\n","train_accuracy = sum(train_accuracy)/len(y_tr)\n","print('Training accuracy:' + train_accuracy)\n","\n","x = x_val\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(np.transpose(w),x)\n","    yp[i] = sigmoid(yp[i])\n","y_val = set(y_val)\n","yp = set(yp)\n","print(yp)\n","y_actual = pd.Series(y_val, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp1 = confmat[1][1]\n","tn1 = confmat[0][0]\n","fp1 = confmat[0][1]\n","fn1 = confmat[1][0]\n","\n","validation_accuracy = (tp1+tn1)/(tp1+tn1+fp1+fn1)\n","print('Validation Accuracy : ' + str(validation_accuracy) + '\\n')\n","\n","x = x_ts\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(np.transpose(w),x)\n","    yp[i] = sigmoid(yp[i])\n","y_ts = set(y_ts)\n","yp = set(yp)\n","print(yp)\n","y_actual = pd.Series(y_ts, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp = confmat[1][1]\n","tn = confmat[0][0]\n","fp = confmat[0][1]\n","fn = confmat[1][0]\n","\n","test_accuracy = (tp+tn)/(tp+tn+fp+fn)\n","sensitivity = tp/(tp+fn)\n","specificity = tn/(tn+fp)\n","\n","print('\\nTest Accuracy : ' + str(test_accuracy))\n","print('sensitivity : ' + str(sensitivity))\n","print('specificity : ' + str(specificity))\n","\n","\n","#Since there are only 100 instances(10 instances for val and 20 for test) \n","#and data.sample sometimes doesn't shuffle the data properly,\n","#it is possible to get an error in confmat index, please run till the shuffling is acceptable"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[ 99.  99.  99.   1.   0.   0. 100.   0.   0. 100.   0. 100.   0.   0.\n"," 100. 100. 100. 100.   0.   0.   0. 100. 100. 100. 100.   0.   0.   0.\n","   0. 100.   0. 100.   0. 100. 100.   0.   0. 100. 100.   0. 100.   0.\n"," 100. 100. 100.   0. 100. 100.   0. 100.   0. 100. 100.   0.   0.   0.\n","   0. 100.   0. 100. 100. 100. 100.   0. 100. 100. 100. 100. 100. 100.]\n","[1.  6.8 2.8 4.8 1.4]\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1b5f5475f930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0myp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_perceptron_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-1b5f5475f930>\u001b[0m in \u001b[0;36mkernel_perceptron_pred\u001b[0;34m(w, y, K, i)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkernel_perceptron_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mhyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (70,) "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mW21ekwClQLi","executionInfo":{"status":"ok","timestamp":1606531363401,"user_tz":-330,"elapsed":1650,"user":{"displayName":"M ABHINAVA","photoUrl":"","userId":"12313623155509412289"}},"outputId":"9c0f6574-0098-4406-e6d8-5d855ead68bf"},"source":["import math \n","import numpy as np\n","import pandas as pd\n","\n","def get_column(data,c):\n","    column = []\n","    for i in range(len(data)):\n","        column.append(data[i][c])\n","    return column\n","\n","def set(a):\n","    for i in range(len(a)):\n","        if a[i] >= 0.5:\n","          a[i] = 1;\n","        else:\n","          a[i] = 0;\n","    return a\n","\n","def sigmoid(x):\n","    return 1 / (1 + math.exp(-x))\n","\n","def poly_kernel(X1, X2):\n","    k = np.matmul(X1 , np.transpose(X2))\n","    k = k + 1\n","    k = np.power(k,3)\n","    return k\n","\n","def kernel_perceptron_pred(w, y, K, i):\n","    hyp = np.multiply(np.multiply(w, y), K[i,:])\n","    hyp = sum(hyp)\n","    if (hyp > 0):\n","          hyp = 1\n","    else:\n","          hyp = -1\n","    return hyp\n","\n","def kernel_perceptron(x_tr, y_tr, iterations):\n","    m = x_tr.shape[0]\n","    n = x_tr.shape[1]\n","    alpha = np.zeros(m)\n","    w = np.zeros(n)\n","    K = poly_kernel(x_tr,x_tr)\n","    hyp = np.zeros(m)\n","    for iter in range(iterations):\n","      for i in range(m):\n","        hyp[i] = kernel_perceptron_pred(alpha, y_tr, K, i)\n","        if (hyp[i]!=y_tr[i]):\n","          alpha[i] = alpha[i] + 1\n","    print('alpha: ' + str(alpha) + '\\n')\n","    for i in range(m):\n","      w += (alpha[i] * y_tr[i]) * x[i,:]\n","    print('weights: ' +str(w))\n","    return w\n","\n","#Normalization\n","def normalize(data):\n","    normalized_data = data\n","    for i in range(5):\n","        sd = np.std(get_column(data,i))\n","        mean = np.mean(get_column(data,i))\n","        for j in range(len(data)):\n","            normalized_data[j][i] = (data[j][i]-mean)/sd\n","    return normalized_data\n","\n","#Data\n","data = pd.read_excel('data55.xlsx',header=None)\n","data = data.sample(frac=1).reset_index(drop=True)\n","#print(data)\n","#print(type(data))\n","data = data.to_numpy()\n","y = data[:,-1]\n","data = normalize(data)\n","x = data[:,:-1]\n","x1 = np.ones(len(x))\n","x = np.column_stack((x1, x))\n","\n","iterations = 100\n","alpha = 0.01\n","\n","#Split into testing and training sets\n","train_size = int(0.7 * len(x))\n","val_limit = int(0.8*len(x))\n","x_tr = x[:train_size]\n","x_val = x[train_size:val_limit]\n","x_ts = x[val_limit:]\n","y_tr = y[:train_size]\n","y_val = y[train_size:val_limit]\n","y_ts = y[val_limit:]\n","\n","yp = np.zeros(len(x_tr))\n","m = x_tr.shape[0]\n","\n","w = kernel_perceptron(x_tr,y_tr, iterations)\n","K = poly_kernel(x_tr, x_tr)\n","\n","x = x_val\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(x[i], np.transpose(w))\n","    yp[i] = sigmoid(yp[i])\n","y_val = set(y_val)\n","yp = set(yp)\n","print('predicted: ' +str(yp) + '\\n')\n","y_actual = pd.Series(y_val, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp1 = confmat[1][1]\n","tn1 = confmat[0][0]\n","fp1 = confmat[0][1]\n","fn1 = confmat[1][0]\n","\n","validation_accuracy = (tp1+tn1)/(tp1+tn1+fp1+fn1)\n","print('Validation Accuracy : ' + str(validation_accuracy) + '\\n')\n","\n","x = x_ts\n","yp = [0 for i in range(len(x))]\n","for i in range(len(x)):\n","    yp[i] = np.matmul(x[i], np.transpose(w))\n","    yp[i] = sigmoid(yp[i])\n","y_ts = set(y_ts)\n","yp = set(yp)\n","print('predicted: ' +str(yp) + '\\n')\n","y_actual = pd.Series(y_ts, name='Actual')\n","y_pred = pd.Series(yp, name='Predicted')\n","confmat = pd.crosstab(y_actual, y_pred)\n","print(confmat)\n","confmat = np.asarray(confmat)\n","tp = confmat[1][1]\n","tn = confmat[0][0]\n","fp = confmat[0][1]\n","fn = confmat[1][0]\n","\n","test_accuracy = (tp+tn)/(tp+tn+fp+fn)\n","sensitivity = tp/(tp+fn)\n","specificity = tn/(tn+fp)\n","\n","print('\\nTest Accuracy : ' + str(test_accuracy))\n","print('sensitivity : ' + str(sensitivity))\n","print('specificity : ' + str(specificity))\n","\n","\n","#Since there are only 100 instances(10 instances for val and 20 for test) \n","#and data.sample sometimes doesn't shuffle the data properly,\n","#it is possible to get an error in confmat index, please run till the shuffling is acceptable"],"execution_count":71,"outputs":[{"output_type":"stream","text":["alpha: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","weights: [ 0.          0.78310769 -1.68893681  1.52639599  1.5973046 ]\n","predicted: [0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n","\n","Predicted  0  1\n","Actual         \n","0.0        6  0\n","1.0        0  4\n","Validation Accuracy : 1.0\n","\n","predicted: [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n","\n","Predicted   0   1\n","Actual           \n","0.0        10   0\n","1.0         0  10\n","\n","Test Accuracy : 1.0\n","sensitivity : 1.0\n","specificity : 1.0\n"],"name":"stdout"}]}]}